#ifndef MAD_MONO_AVX2_TC
#define MAD_MONO_AVX2_TC

/*
 o-----------------------------------------------------------------------------o
 |
 | AVX2 optimization for monimials
 |
 | Methodical Accelerator Design - Copyright CERN 2016+
 | Support: http://cern.ch/mad  - mad at cern.ch
 | Authors: L. Deniau, laurent.deniau at cern.ch
 | Contrib: -
 |
 o-----------------------------------------------------------------------------o
 | You can redistribute this file and/or modify it under the terms of the GNU
 | General Public License GPLv3 (or later), as published by the Free Software
 | Foundation. This file is distributed in the hope that it will be useful, but
 | WITHOUT ANY WARRANTY OF ANY KIND. See http://gnu.org/licenses for details.
 o-----------------------------------------------------------------------------o
 */

#include "mad_sse.h"

int
mad_mono_eq (int n, const ord_t a[n], const ord_t b[n])
{
  assert(a && b);
  int nn=MAD_AVX2_CRND(n), nm=MAD_AVX2_CMOD(n);
  __m256i ra, rb, rr;

  for (int i=0; i < nn; i+=MAD_AVX2_CSIZ) {
    ra = _mm256_loadu_si256((__m256i*)&a[i]);
    rb = _mm256_loadu_si256((__m256i*)&b[i]);
    rr = _mm256_cmpeq_epi8(ra,rb);
    if (~_mm256_movemask_epi8(rr)) return 0;
  }

  if (nm) {
    __m256i rm;
    rm = _mm256_load_si256((__m256i*)mad_avx2_msk2[nm]);
    ra = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&a[nn]));
    rb = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&b[nn]));
    rr = _mm256_cmpeq_epi8(ra,rb);
    if (~_mm256_movemask_epi8(rr)) return 0;
  }

  return 1;
}

int
mad_mono_lt (int n, const ord_t a[n], const ord_t b[n])
{
  assert(a && b);
  int nn=MAD_AVX2_CRND(n), nm=MAD_AVX2_CMOD(n);
  __m256i ra, rb, rr, re;

  for (int i=0; i < nn; i+=MAD_AVX2_CSIZ) {
    ra = _mm256_loadu_si256((__m256i*)&a[i]);
    rb = _mm256_loadu_si256((__m256i*)&b[i]);
    re = _mm256_cmpeq_epi8(ra,rb);
    rr = _mm256_cmpgt_epi8(ra,rb);
    if (_mm256_movemask_epi8(rr) | _mm256_movemask_epi8(re)) return 0;
  }

  if (nm) {
    __m256i rm;
    rm = _mm256_load_si256((__m256i*)mad_avx2_msk2[nm]);
    ra = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&a[nn]));
    rb = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&b[nn]));
    re = _mm256_cmpeq_epi8(ra,rb);
    rr = _mm256_cmpgt_epi8(ra,rb);
    if (_mm256_movemask_epi8(rr) | _mm256_movemask_epi8(re)) return 0;
  }

  return 1;
}

int
mad_mono_le (int n, const ord_t a[n], const ord_t b[n])
{
  assert(a && b);
  int nn=MAD_AVX2_CRND(n), nm=MAD_AVX2_CMOD(n);
  __m256i ra, rb, rr;

  for (int i=0; i < nn; i+=MAD_AVX2_CSIZ) {
    ra = _mm256_loadu_si256((__m256i*)&a[i]);
    rb = _mm256_loadu_si256((__m256i*)&b[i]);
    rr = _mm256_cmpgt_epi8(ra,rb);
    if (_mm256_movemask_epi8(rr)) return 0;
  }

  if (nm) {
    __m256i rm;
    rm = _mm256_load_si256((__m256i*)mad_avx2_msk2[nm]);
    ra = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&a[nn]));
    rb = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&b[nn]));
    rr = _mm256_cmpgt_epi8(ra,rb);
    if (_mm256_movemask_epi8(rr)) return 0;
  }

  return 1;
}

int
mad_mono_rcmp (int n, const ord_t a[n], const ord_t b[n])
{
  assert(a && b);
  int nn=MAD_AVX2_CRND(n), nm=MAD_AVX2_CMOD(n);
  __m256i ra, rb, rr;

  if (nm) {
    __m256i rm;
    rm = _mm256_load_si256((__m256i*)mad_avx2_msk2[nm]);
    ra = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&a[nn]));
    rb = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&b[nn]));
    rr = _mm256_cmpeq_epi8(ra,rb);
    if (~_mm256_movemask_epi8(rr))
      for (int i=nm-1; i >= 0; i--)
        if (!((ord_t*)&rr)[i]) return ((ord_t*)&ra)[i] - ((ord_t*)&rb)[i];
  }

  for (int i=nn-MAD_AVX2_CSIZ; i >= 0; i-=MAD_AVX2_CSIZ) {
    ra = _mm256_loadu_si256((__m256i*)&a[i]);
    rb = _mm256_loadu_si256((__m256i*)&b[i]);
    rr = _mm256_cmpeq_epi8(ra,rb);
    if (~_mm256_movemask_epi8(rr))
      for (int i=MAD_AVX2_CSIZ-2; i >= 0; i-=2) {
        if (!((ord_t*)&rr)[i+1]) return ((ord_t*)&ra)[i+1] - ((ord_t*)&rb)[i+1];
        if (!((ord_t*)&rr)[i+0]) return ((ord_t*)&ra)[i+0] - ((ord_t*)&rb)[i+0];
      }
  }

  return 0;
}


ord_t
mad_mono_min (int n, const ord_t a[n])
{
  assert(a);
  int nn=MAD_AVX2_CRND(n), nm=MAD_AVX2_CMOD(n);
  __m256i ra, rr = _mm256_set1_epi8(-1);

  for (int i=0; i < nn; i+=MAD_AVX2_CSIZ) {
    ra = _mm256_loadu_si256((__m256i*)&a[i]);
    rr = _mm256_min_epi8(rr,ra);
  }

  if (nm) {
    __m256i rm;
    rm = _mm256_load_si256((__m256i*)mad_avx2_msk2[nm]);
    ra = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&a[nn]));
    rr = _mm256_min_epi8(rr,ra);
  }

  int ni = nn ? MAD_AVX2_CSIZ : nm;
  ord_t m0 = -1, m1 = -1;
  for (int i=0; i < ni; i+=2) {
    if (m0 > ((ord_t*)&rr)[i+0]) m0 = ((ord_t*)&rr)[i+0];
    if (m1 > ((ord_t*)&rr)[i+1]) m1 = ((ord_t*)&rr)[i+1];
  }

  return m0 <= m1 ? m0 : m1;
}

ord_t
mad_mono_max (int n, const ord_t a[n])
{
  assert(a);
  int nn=MAD_AVX2_CRND(n), nm=MAD_AVX2_CMOD(n);
  __m256i ra, rr = _mm256_setzero_si256();

  for (int i=0; i < nn; i+=MAD_AVX2_CSIZ) {
    ra = _mm256_loadu_si256((__m256i*)&a[i]);
    rr = _mm256_max_epi8(rr,ra);
  }

  if (nm) {
    __m256i rm;
    rm = _mm256_load_si256((__m256i*)mad_avx2_msk2[nm]);
    ra = _mm256_and_si256(rm,_mm256_loadu_si256((__m256i*)&a[nn]));
    rr = _mm256_max_epi8(rr,ra);
  }

  int ni = nn ? MAD_AVX2_CSIZ : nm;
  ord_t m0 = 0, m1 = 0;
  for (int i=0; i < ni; i+=2) {
    if (m0 < ((ord_t*)&rr)[i+0]) m0 = ((ord_t*)&rr)[i+0];
    if (m1 < ((ord_t*)&rr)[i+1]) m1 = ((ord_t*)&rr)[i+1];
  }

  return m0 >= m1 ? m0 : m1;
}

// --- end --------------------------------------------------------------------o

#endif // MAD_MONO_AVX2_TC
